{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(filename):  #解pickle数据\n",
    "    import pickle\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataSet = pickle.load(f, encoding = 'bytes')\n",
    "    return dataSet\n",
    "\n",
    "def splitData(dataSet):\n",
    "    trainSet = dataSet[b'data']\n",
    "    trainLabel = dataSet[b'labels']\n",
    "    return trainSet, np.mat(trainLabel).T\n",
    "\n",
    "def visualSomeImage(trainSet, trainLabel, labelNames):  #可视化部分图形\n",
    "    trainLabel = np.array(trainLabel)\n",
    "    numClass = len(labelNames)\n",
    "    sample_per_class = 7   #在一张图片里可视化7x10\n",
    "    for y, cls in enumerate(labelNames):\n",
    "        idxs = np.flatnonzero(trainLabel == y)\n",
    "        idxs = np.random.choice(idxs, sample_per_class, replace = False)\n",
    "        for i, idx in enumerate(idxs):\n",
    "            plt_idx = i * numClass + y + 1\n",
    "            plt.subplot(sample_per_class, numClass, plt_idx)\n",
    "            image = trainSet[idx].reshape(3, 32, 32).transpose(1, 2, 0).astype('float')\n",
    "            plt.imshow(image.astype('uint8'))\n",
    "            plt.axis('off')\n",
    "            if i == 0:\n",
    "                plt.title(cls)\n",
    "    plt.show()\n",
    "                \n",
    "\n",
    "def get_All_Data():  #加载图片数据集，并且划分为训练集和测试集（首先并没有划分dev集）\n",
    "    trainFile1 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_1'  #其中之一的训练集\n",
    "    trainFile2 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_2'  #其中之一的训练集\n",
    "    trainFile3 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_3'  #其中之一的训练集\n",
    "    trainFile4 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_4'  #其中之一的训练集\n",
    "    trainFile5 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_5'  #其中之一的训练集\n",
    "    labelFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\batches.meta'  #返回的的是下标的英文名字\n",
    "    testFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\test_batch'  #测试集合\n",
    "\n",
    "    dataSet = unpickle(trainFile1)  #首先使用的是一个数据集，看看效果\n",
    "    trainSet1, trainLabel1 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile2)  \n",
    "    trainSet2, trainLabel2 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile3)  \n",
    "    trainSet3, trainLabel3 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile4)  \n",
    "    trainSet4, trainLabel4 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile5)  \n",
    "    trainSet5, trainLabel5 = splitData(dataSet)   #训练集\n",
    "\n",
    "    trainSet = np.concatenate((trainSet1, trainSet2), axis = 0)\n",
    "    trainLabel = np.concatenate((trainLabel1, trainLabel2), axis = 0)\n",
    "    dataSet = unpickle(testFile)\n",
    "    testSet, testLabel = splitData(dataSet)  #测试集\n",
    "    \n",
    "    #随机选取1000张图片作测试，看准确度\n",
    "    '''\n",
    "    randIndex = np.random.permutation(10000)\n",
    "    shuffled_X = testAllSet[randIndex, :]\n",
    "    shuffled_Y = testAllLabel[randIndex, :]\n",
    "    testSet = shuffled_X[:1000, :]   #随机取1000个数据集作为测试\n",
    "    testLabel = shuffled_Y[:1000, :]\n",
    "    '''\n",
    "    label = unpickle(labelFile)  \n",
    "    labelNames = label[b'label_names'] #label的名字,例如'cat'\n",
    "    #visualSomeImage(trainSet, trainLabel, labelNames)  #可视化\n",
    "\n",
    "    #normlization\n",
    "    trainSet, testSet = trainSet.astype(float), testSet.astype(float)\n",
    "    mean_image = np.mean(trainSet, axis = 0)\n",
    "    trainSet -= np.mat(mean_image)\n",
    "    testSet -= mean_image\n",
    "\n",
    "    return trainSet1, trainLabel1, testSet, testLabel, labelNames\n",
    "    \n",
    "def miniBatch(trainSet, trainLabel,  batchSize = 256):  #将训练数据划分为minibatch，然后进行训练\n",
    "    m = trainSet.shape[0] #训练集的总数\n",
    "    mini_batches = []  #划分训练集存储的地方\n",
    "\n",
    "    #先打乱顺序，然后再mini-batch\n",
    "    randIndex = np.random.permutation(m)\n",
    "    shuffled_X = trainSet[randIndex, :]\n",
    "    shuffled_Y = trainLabel[randIndex, :]\n",
    "\n",
    "    num_complete_minibatches = int(m/batchSize)\n",
    "    for k in range(num_complete_minibatches):\n",
    "        minibatch_X = shuffled_X[k*batchSize : (k+1)*batchSize, :]\n",
    "        minibatch_Y = shuffled_Y[k*batchSize : (k+1)*batchSize, :]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        mini_batches.append(minibatch)\n",
    "\n",
    "    if m % batchSize != 0:  #如果数据不是minibatch的整数倍，对剩下的单独处理\n",
    "        minibatch_X = shuffled_X[num_complete_minibatches*batchSize:, :]\n",
    "        minibatch_Y = shuffled_Y[num_complete_minibatches*batchSize:, :]\n",
    "        mini_batches.append((minibatch_X, minibatch_Y))\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def convert_to_one_hot(Y, C):  #将标签变为数量乘以10（这里有10类）\n",
    "    Y = np.eye(C)[Y].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():  #吴恩达老师的手势数据集\n",
    "    train_dataset = h5py.File(r'E:\\jupyter notebook\\finger_sign_dataSet\\train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(r'E:\\jupyter notebook\\finger_sign_dataSet\\test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "trainSet, testSet = train_set_x_orig/255, test_set_x_orig/255\n",
    "train_set_y_orig = np.array(list(map(int, train_set_y_orig.reshape(-1, 1))))\n",
    "test_set_y_orig = np.array(list(map(int, test_set_y_orig.reshape(-1, 1))))\n",
    "trainLabel = convert_to_one_hot(train_set_y_orig, 6).T\n",
    "testLabel = convert_to_one_hot(test_set_y_orig, 6).T\n",
    "m = trainLabel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainFile1 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_1'  #其中之一的训练集\n",
    "labelFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\batches.meta'  #返回的的是下标的英文名字\n",
    "\n",
    "dataSet = unpickle(trainFile1)  #首先使用的是一个数据集，看看效果\n",
    "trainSet, trainLabel = splitData(dataSet)   #训练集\n",
    "label = unpickle(labelFile)  \n",
    "labelNames = label[b'label_names'] #label的名字,例如'cat'\n",
    "\n",
    "testFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\test_batch'  #测试集合\n",
    "dataSet = unpickle(testFile)  #首先使用的是一个数据集，看看效果\n",
    "testSet, testLabel = splitData(dataSet)   #训练集\n",
    "m = trainSet.shape[0]\n",
    "\n",
    "trainSet = trainSet.reshape(m, 3, 32, 32).transpose(0, 2, 3, 1)/255  #将数据变为图片形式，然后输入训练,记住需要归一化\n",
    "trainLabel = np.array(list(map(int, trainLabel)))\n",
    "trainLabel = convert_to_one_hot(trainLabel, 10).T\n",
    "\n",
    "testSet = testSet.reshape(m, 3, 32, 32).transpose(0, 2, 3, 1)/255  #将数据变为图片形式，然后输入训练,记住需要归一化\n",
    "testLabel = np.array(list(map(int, testLabel)))\n",
    "testLabel = convert_to_one_hot(testLabel, 10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(16, (3, 3), padding='same', input_shape=(64, 64, 3)))\n",
    "#model.add(BatchNormalization(axis = 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='valid'))\n",
    "#model.add(BatchNormalization(axis = 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='valid'))\n",
    "#model.add(BatchNormalization(axis = 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "opti = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1080/1080 [==============================] - 4s - loss: 0.2122 - acc: 0.9250     \n",
      "Epoch 2/4\n",
      "1080/1080 [==============================] - 4s - loss: 0.1657 - acc: 0.9463     \n",
      "Epoch 3/4\n",
      "1080/1080 [==============================] - 4s - loss: 0.1786 - acc: 0.9426     \n",
      "Epoch 4/4\n",
      "1080/1080 [==============================] - 4s - loss: 0.1414 - acc: 0.9593     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xaf93d79a20>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainSet, trainLabel, batch_size=64, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testSet, testLabel, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18709770838419595, 0.94166667858759567]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save(r'E:\\jupyter notebook\\store_model\\storeModel.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
