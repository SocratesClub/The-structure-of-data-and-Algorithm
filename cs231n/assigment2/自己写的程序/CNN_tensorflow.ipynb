{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "和吴恩达老师不同，这里的数据是行向量的，即每列表示对应的特征，行表示由多少个数据\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(filename):  #解pickle数据\n",
    "    import pickle\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataSet = pickle.load(f, encoding = 'bytes')\n",
    "    return dataSet\n",
    "\n",
    "def splitData(dataSet):\n",
    "    trainSet = dataSet[b'data']\n",
    "    trainLabel = dataSet[b'labels']\n",
    "    return trainSet, np.mat(trainLabel).T\n",
    "\n",
    "def visualSomeImage(trainSet, trainLabel, labelNames):  #可视化部分图形\n",
    "    trainLabel = np.array(trainLabel)\n",
    "    numClass = len(labelNames)\n",
    "    sample_per_class = 7   #在一张图片里可视化7x10\n",
    "    for y, cls in enumerate(labelNames):\n",
    "        idxs = np.flatnonzero(trainLabel == y)\n",
    "        idxs = np.random.choice(idxs, sample_per_class, replace = False)\n",
    "        for i, idx in enumerate(idxs):\n",
    "            plt_idx = i * numClass + y + 1\n",
    "            plt.subplot(sample_per_class, numClass, plt_idx)\n",
    "            image = trainSet[idx].reshape(3, 32, 32).transpose(1, 2, 0).astype('float')\n",
    "            plt.imshow(image.astype('uint8'))\n",
    "            plt.axis('off')\n",
    "            if i == 0:\n",
    "                plt.title(cls)\n",
    "    plt.show()\n",
    "                \n",
    "\n",
    "def get_All_Data():  #加载图片数据集，并且划分为训练集和测试集（首先并没有划分dev集）\n",
    "    trainFile1 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_1'  #其中之一的训练集\n",
    "    trainFile2 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_2'  #其中之一的训练集\n",
    "    trainFile3 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_3'  #其中之一的训练集\n",
    "    trainFile4 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_4'  #其中之一的训练集\n",
    "    trainFile5 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_5'  #其中之一的训练集\n",
    "    labelFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\batches.meta'  #返回的的是下标的英文名字\n",
    "    testFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\test_batch'  #测试集合\n",
    "\n",
    "    dataSet = unpickle(trainFile1)  #首先使用的是一个数据集，看看效果\n",
    "    trainSet1, trainLabel1 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile2)  \n",
    "    trainSet2, trainLabel2 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile3)  \n",
    "    trainSet3, trainLabel3 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile4)  \n",
    "    trainSet4, trainLabel4 = splitData(dataSet)   #训练集\n",
    "    dataSet = unpickle(trainFile5)  \n",
    "    trainSet5, trainLabel5 = splitData(dataSet)   #训练集\n",
    "\n",
    "    trainSet = np.concatenate((trainSet1, trainSet2), axis = 0)\n",
    "    trainLabel = np.concatenate((trainLabel1, trainLabel2), axis = 0)\n",
    "    dataSet = unpickle(testFile)\n",
    "    testSet, testLabel = splitData(dataSet)  #测试集\n",
    "    \n",
    "    #随机选取1000张图片作测试，看准确度\n",
    "    '''\n",
    "    randIndex = np.random.permutation(10000)\n",
    "    shuffled_X = testAllSet[randIndex, :]\n",
    "    shuffled_Y = testAllLabel[randIndex, :]\n",
    "    testSet = shuffled_X[:1000, :]   #随机取1000个数据集作为测试\n",
    "    testLabel = shuffled_Y[:1000, :]\n",
    "    '''\n",
    "    label = unpickle(labelFile)  \n",
    "    labelNames = label[b'label_names'] #label的名字,例如'cat'\n",
    "    #visualSomeImage(trainSet, trainLabel, labelNames)  #可视化\n",
    "\n",
    "    #normlization\n",
    "    trainSet, testSet = trainSet.astype(float), testSet.astype(float)\n",
    "    mean_image = np.mean(trainSet, axis = 0)\n",
    "    trainSet -= np.mat(mean_image)\n",
    "    testSet -= mean_image\n",
    "\n",
    "    return trainSet1, trainLabel1, testSet, testLabel, labelNames\n",
    "    \n",
    "def miniBatch(trainSet, trainLabel,  batchSize = 256):  #将训练数据划分为minibatch，然后进行训练\n",
    "    m = trainSet.shape[0] #训练集的总数\n",
    "    mini_batches = []  #划分训练集存储的地方\n",
    "\n",
    "    #先打乱顺序，然后再mini-batch\n",
    "    randIndex = np.random.permutation(m)\n",
    "    shuffled_X = trainSet[randIndex, :]\n",
    "    shuffled_Y = trainLabel[randIndex, :]\n",
    "\n",
    "    num_complete_minibatches = int(m/batchSize)\n",
    "    for k in range(num_complete_minibatches):\n",
    "        minibatch_X = shuffled_X[k*batchSize : (k+1)*batchSize, :]\n",
    "        minibatch_Y = shuffled_Y[k*batchSize : (k+1)*batchSize, :]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        mini_batches.append(minibatch)\n",
    "\n",
    "    if m % batchSize != 0:  #如果数据不是minibatch的整数倍，对剩下的单独处理\n",
    "        minibatch_X = shuffled_X[num_complete_minibatches*batchSize:, :]\n",
    "        minibatch_Y = shuffled_Y[num_complete_minibatches*batchSize:, :]\n",
    "        mini_batches.append((minibatch_X, minibatch_Y))\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def convert_to_one_hot(Y, C):  #将标签变为数量乘以10（这里有10类）\n",
    "    Y = np.eye(C)[Y].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():  #吴恩达老师的手势数据集\n",
    "    train_dataset = h5py.File(r'E:\\jupyter notebook\\finger_sign_dataSet\\train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(r'E:\\jupyter notebook\\finger_sign_dataSet\\test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "trainSet, testSet = train_set_x_orig/255, test_set_x_orig/255\n",
    "train_set_y_orig = np.array(list(map(int, train_set_y_orig.reshape(-1, 1))))\n",
    "test_set_y_orig = np.array(list(map(int, test_set_y_orig.reshape(-1, 1))))\n",
    "trainLabel = convert_to_one_hot(train_set_y_orig, 6).T\n",
    "testLabel = convert_to_one_hot(test_set_y_orig, 6).T\n",
    "m = trainLabel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainFile1 = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\data_batch_1'  #其中之一的训练集\n",
    "labelFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\batches.meta'  #返回的的是下标的英文名字\n",
    "\n",
    "dataSet = unpickle(trainFile1)  #首先使用的是一个数据集，看看效果\n",
    "trainSet, trainLabel = splitData(dataSet)   #训练集\n",
    "label = unpickle(labelFile)  \n",
    "labelNames = label[b'label_names'] #label的名字,例如'cat'\n",
    "\n",
    "testFile = r'F:\\数据集\\斯坦福大学数据集\\cifar-10-python\\cifar-10-batches-py\\test_batch'  #测试集合\n",
    "dataSet = unpickle(testFile)  #首先使用的是一个数据集，看看效果\n",
    "testSet, testLabel = splitData(dataSet)   #训练集\n",
    "m = trainSet.shape[0]\n",
    "\n",
    "trainSet = trainSet.reshape(m, 3, 32, 32).transpose(0, 2, 3, 1)/255  #将数据变为图片形式，然后输入训练,记住需要归一化\n",
    "trainLabel = np.array(list(map(int, trainLabel)))\n",
    "trainLabel = convert_to_one_hot(trainLabel, 10).T\n",
    "\n",
    "testSet = testSet.reshape(m, 3, 32, 32).transpose(0, 2, 3, 1)/255  #将数据变为图片形式，然后输入训练,记住需要归一化\n",
    "testLabel = np.array(list(map(int, testLabel)))\n",
    "testLabel = convert_to_one_hot(testLabel, 10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholder(n_H, n_W, n_C, n_y):  #创建tensorflow的placeholder\n",
    "    X = tf.placeholder(tf.float32, [None, n_H, n_W, n_C])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "    \n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters():  #这里采用的是固定的filter，然后进行运算，我采用2层卷积网络\n",
    "    \"\"\"\n",
    "    W1 = [4, 4, 3, 8]\n",
    "    W2 = [2, 2, 8, 16]\n",
    "    W3 = [2, 2, 16, 32]\n",
    "    \"\"\"\n",
    "    W1 = tf.get_variable('W1', [4, 4, 3, 8], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable('W2', [2, 2, 8, 16], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable('W3', [2, 2, 16, 32], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \n",
    "    parameters = {'W1':W1,\n",
    "                 'W2':W2,\n",
    "                 'W3':W3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):  #前向传播\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    \n",
    "    Z1 = tf.nn.conv2d(X, W1, strides = (1, 1, 1, 1), padding = 'SAME')\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize = (1, 4, 4, 1), strides = (1, 4, 4, 1), padding = 'SAME')\n",
    "    \n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides = (1, 1, 1, 1), padding = 'SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2, ksize = (1, 2, 2, 1), strides = (1, 2, 2, 1), padding = 'SAME')\n",
    "\n",
    "    Z3 = tf.nn.conv2d(P2, W3, strides = (1, 1, 1, 1), padding = 'SAME')\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    P3 = tf.nn.max_pool(A3, ksize = (1, 2, 2 ,1), strides = (1, 2, 2, 1), padding = 'SAME')\n",
    "\n",
    "    P3 = tf.contrib.layers.flatten(P3)\n",
    "    \n",
    "    Z4 = tf.contrib.layers.fully_connected(P3, 10, activation_fn = None)\n",
    "    \n",
    "    return Z4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(Z4, Y):  #计算损失\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z4, labels = Y)) #x这里的Z4一定要是原始的，因为这里的函数有softmax\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(trainSet, trainLabel, learning_rate = 0.009, num_epochs = 100, minibatch_size = 256):#融合模型\n",
    "    ops.reset_default_graph()  \n",
    "    \n",
    "    m, n_H, n_W, n_C = trainSet.shape\n",
    "    n_y = trainLabel.shape[1]\n",
    "    costs = []  #存储总的cost，来画图\n",
    "    \n",
    "    X, Y = create_placeholder(n_H, n_W, n_C, n_y)\n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    Z4 = forward_propagation(X, parameters)\n",
    "    loss = compute_loss(Z4, Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(m/minibatch_size)\n",
    "            minibatches = miniBatch(trainSet, trainLabel, batchSize = minibatch_size)\n",
    "            for minibatch in minibatches:\n",
    "                minibatch_X, minibatch_Y = minibatch\n",
    "                _ , temp_cost = sess.run([optimizer, loss], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                minibatch_cost += temp_cost/num_minibatches\n",
    "                \n",
    "            if epoch % 5 == 0:\n",
    "                print('after epoch %d, loss is %f' %(epoch, minibatch_cost))\n",
    "            if epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, 'E:/jupyter notebook/store_model/save_net.ckpt')\n",
    "        '''\n",
    "        predict_op = tf.argmax(Z4, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        #print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: trainSet, Y: trainLabel})\n",
    "        print(\"Train Accuracy:\", train_accuracy)     \n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after epoch 0, loss is 2.180498\n",
      "after epoch 5, loss is 1.610578\n",
      "after epoch 10, loss is 1.490229\n",
      "after epoch 15, loss is 1.407680\n",
      "after epoch 20, loss is 1.342103\n",
      "after epoch 25, loss is 1.287238\n",
      "after epoch 30, loss is 1.250521\n",
      "after epoch 35, loss is 1.206639\n",
      "after epoch 40, loss is 1.186369\n",
      "after epoch 45, loss is 1.149575\n",
      "after epoch 50, loss is 1.134303\n",
      "after epoch 55, loss is 1.102972\n",
      "after epoch 60, loss is 1.086617\n",
      "after epoch 65, loss is 1.066568\n",
      "after epoch 70, loss is 1.047632\n",
      "after epoch 75, loss is 1.040087\n",
      "after epoch 80, loss is 1.019918\n",
      "after epoch 85, loss is 1.006820\n",
      "after epoch 90, loss is 1.001028\n",
      "after epoch 95, loss is 0.988559\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xede81ff9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(trainSet, trainLabel, learning_rate = 0.001, num_epochs = 100, minibatch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predSelf(testSet, testLabel = None):  #预测，存储之后的值进行预测\n",
    "    ops.reset_default_graph()\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        parameters = initialize_parameters()\n",
    "        (m, n_H0, n_W0, n_C0) = testSet.shape  \n",
    "        n_y = testLabel.shape[1]                            \n",
    "        X, Y = tf.placeholder(tf.float32, [None,  n_H0, n_W0, n_C0]), tf.placeholder(tf.float32, [None, n_y])\n",
    "        Z4 = forward_propagation(X, parameters)\n",
    "\n",
    "        # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, 'E:/jupyter notebook/store_model/save_net.ckpt')\n",
    "\n",
    "        predict_op = tf.argmax(Z4, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        #print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: testSet, Y: testLabel})\n",
    "        print(\"Test Accuracy:\", train_accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from E:/jupyter notebook/store_model/save_net.ckpt\n",
      "Test Accuracy: 0.537\n"
     ]
    }
   ],
   "source": [
    "#testSet.shape\n",
    "#predSelf(trainSet, trainLabel)\n",
    "predSelf(testSet[:1000], testLabel[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "之前没有归一化，各种错误。。。梯度不下降了。。记住归一化的重要性\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15411\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  #这个一定要，不然就会出现什么重复出现什么的\n",
    "with tf.Session() as sess:  #最好用这种情形吧，可以使用eval()\n",
    "    parameters = initialize_parameters()   #参数要在Init上面，不然未定义\n",
    "    X, Y = create_placeholder(32, 32, 3, 10)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    loss = compute_loss(Z3,Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(loss, feed_dict = {X:np.random.randn(4, 32, 32, 3), Y:np.random.randn(4, 10)}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
